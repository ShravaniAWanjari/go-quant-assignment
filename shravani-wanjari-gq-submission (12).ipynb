{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":108376,"databundleVersionId":13345061,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I started by dissecting the starter notebook, and I immediately saw some red flags.\n\nUsing a simple linear model for what I knew was a non-linear problem, the naive fillna(0), and the critical error of randomly splitting time-series data were all things I flagged to fix.\n\nMy initial thinking on feature engineering was to use a 10s moving average, but after talking to Gemini, I realized a longer window (like 600s) would be better to capture the signal through the noise. \n\nThe more professional approach I landed on was to engineer multiple windows (10s, 60s, 600s) and feed them all to the model.\n\nThe real challenge began when I tried to process the full dataset. \n\nThe kernel crashed repeatedly under the memory pressure, even when I tried to be clever with float32 types (which caused overflows) or iterative processing. This was a classic data pipeline bottleneck. \n\nIt forced a strategic pivot away from trying to hold one giant table in memory.But this whole thing hit a big roadblock when I scored -0.00170611 T_T\n\nThe solution was to stop thinking about merging raw data and start thinking about merging features. I built a pipeline to process each asset file independently, calculate a rich set of features (returns, realized volatility, spread, imbalance), and save the results to a lightweight cache. \n\nThis finally solved the memory issue and let the real analysis begin.\n\nWith the data accessible, EDA uncovered two crucial, opposing facts:\n\nStrong Autocorrelation: ETH's own implied volatility was highly persistent. Its past was a very strong predictor of its future.\n\nCritical Anomaly: ETH's price returns showed almost zero correlation with the rest of the market, including BTC. This was a huge surprise and suggested a major data quality issue that made direct cross-asset prediction unreliable.\n\nThis forced my final, most important strategic pivot. Instead of using other assets to predict ETH directly, \n\nI decided to use them to create \"market context\" features.\n\nAggregated the volatility and spread from all other assets into market-wide indicators.I ended the day by building the final data pipeline and creating a validation plot that proved the hypothesis. \n\nThe plot clearly showed ETH's implied volatility spiking dramatically at the exact same time the overall market entered a high-stress state.\n\nThe data part is now done. I have a robust, feature-rich, and validated dataset ready for modeling.\n\nUsed LightBGM and \nAfter hitting a solid 0.65 Pearson correlation, the next phase was a deep dive into iterative feature engineering to push the score higher. I experimented with more complex features, like volatility ratios and advanced order book metrics, but found they didn't provide a significant lift. This process was crucial as it proved that our initial, simpler feature set was the most robust and signal-rich. With the champion features locked in, I used Optuna to hyperparameter-tune the LightGBM model, squeezing out the maximum performance. This disciplined approach of feature validation followed by tuning resulted in our final, reliable out-of-fold correlation of 0.733 and a complete end-to-end pipeline.\n\n","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n#\n# DEEP HOLDOUT VALIDATION SCRIPT\n#\n# =============================================================================\n\n# =============================================================================\n# 1. IMPORTS & SETUP (Same as before)\n# =============================================================================\nimport pandas as pd\nimport numpy as np\nimport glob\nfrom pathlib import Path\nimport lightgbm as lgb\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import pearsonr\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nimport matplotlib.pyplot as plt\nimport gc\n\nprint(\"--- Setup Complete ---\")\n\n# --- Configuration ---\nDATA_DIR = Path(\"/kaggle/input/gq-implied-volatility-forecasting\")\nOUTPUT_DIR = Path(\"/kaggle/working/\")\n\n\n# =============================================================================\n# 2. CHAMPION FEATURE & DATA PREPARATION (Same as before)\n# =============================================================================\n\ndef calculate_champion_features(df: pd.DataFrame) -> pd.DataFrame:\n    # This function is identical to the winning version from Phase 2\n    df = df.sort_index()\n    price_cols = ['mid_price', 'ask_price1', 'bid_price1']\n    for col in price_cols:\n        if col in df.columns:\n            df.loc[:, col] = df[col].replace(0, np.nan)\n    df['wap'] = (df['bid_price1'] * df['ask_volume1'] + df['ask_price1'] * df['bid_volume1']) / (df['bid_volume1'] + df['ask_volume1'])\n    df['log_return_wap_1s'] = np.log(df['wap']).diff(1)\n    weights = [0.4, 0.3, 0.15, 0.1, 0.05]\n    df['weighted_bid_vol'] = (weights * df[[f'bid_volume{i}' for i in range(1, 6)]]).sum(axis=1)\n    df['weighted_ask_vol'] = (weights * df[[f'ask_volume{i}' for i in range(1, 6)]]).sum(axis=1)\n    df['depth_weighted_imbalance'] = (df['weighted_bid_vol'] - df['weighted_ask_vol']) / (df['weighted_bid_vol'] + df['weighted_ask_vol'])\n    df['log_return_1s'] = np.log(df['mid_price']).diff(1)\n    for n in [10, 30, 60, 600]:\n        df[f'realized_vol_{n}s'] = df['log_return_wap_1s'].rolling(window=n).std()\n    df['wap_trend_300s'] = df['wap'].diff(periods=300)\n    df['spread'] = df['ask_price1'] - df['bid_price1']\n    bid_vols = df[[f'bid_volume{i}' for i in range(1, 6)]].sum(axis=1)\n    ask_vols = df[[f'ask_volume{i}' for i in range(1, 6)]].sum(axis=1)\n    df['book_imbalance'] = (bid_vols - ask_vols) / (bid_vols + ask_vols)\n    df['vol_of_vol_10s_w60'] = df['realized_vol_10s'].rolling(window=60).std()\n    df['imbalance_momentum_5s'] = df['book_imbalance'].diff(periods=5)\n    feature_cols = [\n        'log_return_1s', 'log_return_wap_1s', 'realized_vol_10s', 'realized_vol_30s',\n        'realized_vol_60s', 'realized_vol_600s', 'wap_trend_300s', 'spread',\n        'book_imbalance', 'depth_weighted_imbalance', 'vol_of_vol_10s_w60', 'imbalance_momentum_5s'\n    ]\n    return df[feature_cols].copy()\n\nprint(\"âœ… Phase 1 Setup Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T03:55:23.340843Z","iopub.execute_input":"2025-08-15T03:55:23.341087Z","iopub.status.idle":"2025-08-15T03:55:43.685599Z","shell.execute_reply.started":"2025-08-15T03:55:23.341067Z","shell.execute_reply":"2025-08-15T03:55:43.684945Z"}},"outputs":[{"name":"stderr","text":"2025-08-15 03:55:31.653217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755230131.877539      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755230131.939551      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"--- Setup Complete ---\nâœ… Phase 1 Setup Complete\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def create_full_feature_set(data_dir):\n    # This is a simplified version of the data prep function for the train set\n    print(\"--- Processing Training Data ---\")\n    # (The logic here is the same as your Phase 1 data prep)\n    # ... condensed for brevity but the logic is identical ...\n    data_type = \"train\"\n    all_asset_paths = glob.glob(str(data_dir / f\"{data_type}/*.csv\"))\n    all_possible_cols = ['timestamp', 'mid_price'] + [f'{side}_{level}{i}' for side in ['bid', 'ask'] for level in ['price', 'volume'] for i in range(1, 6)] + ['label']\n    eth_path = [p for p in all_asset_paths if 'ETH' in p][0]\n    with open(eth_path, 'r') as f: header = f.readline().strip().split(',')\n    cols_to_load = [col for col in all_possible_cols if col in header]\n    df_eth_raw = pd.read_csv(eth_path, usecols=cols_to_load)\n    df_eth_raw['timestamp'] = pd.to_datetime(df_eth_raw['timestamp'], format='mixed', utc=True)\n    df_eth_raw = df_eth_raw.set_index('timestamp').sort_index()\n    df_eth_raw = df_eth_raw[~df_eth_raw.index.duplicated(keep='first')]\n    eth_features = calculate_champion_features(df_eth_raw)\n    eth_df = df_eth_raw[['label']].join(eth_features)\n    cross_asset_features_list = []\n    for asset_path in all_asset_paths:\n        if 'ETH' in asset_path or 'submission' in asset_path: continue\n        with open(asset_path, 'r') as f: header = f.readline().strip().split(',')\n        cols_to_load = [col for col in all_possible_cols if col in header]\n        df_asset_raw = pd.read_csv(asset_path, usecols=cols_to_load)\n        df_asset_raw['timestamp'] = pd.to_datetime(df_asset_raw['timestamp'], format='mixed', utc=True)\n        df_asset_raw = df_asset_raw.set_index('timestamp').sort_index()\n        df_asset_raw = df_asset_raw[~df_asset_raw.index.duplicated(keep='first')]\n        market_features = calculate_champion_features(df_asset_raw)\n        cross_asset_features_list.append(market_features)\n    market_df = pd.concat(cross_asset_features_list, axis=1)\n    market_context_df = pd.DataFrame(index=market_df.index)\n    market_context_df['market_realized_vol_60s'] = market_df.filter(like='realized_vol_60s').mean(axis=1)\n    market_context_df['market_depth_weighted_imbalance'] = market_df.filter(like='depth_weighted_imbalance').mean(axis=1)\n    final_df = eth_df.join(market_context_df, how='left')\n    final_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    final_df = final_df.ffill().fillna(0)\n    print(f\"âœ… Final feature set created. Shape: {final_df.shape}\")\n    return final_df\n\ntrain_df = create_full_feature_set(DATA_DIR)\nX = train_df.drop('label', axis=1)\ny = train_df['label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T03:55:43.686850Z","iopub.execute_input":"2025-08-15T03:55:43.687425Z","iopub.status.idle":"2025-08-15T03:56:04.898283Z","shell.execute_reply.started":"2025-08-15T03:55:43.687405Z","shell.execute_reply":"2025-08-15T03:56:04.897465Z"}},"outputs":[{"name":"stdout","text":"--- Processing Training Data ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"âœ… Final feature set created. Shape: (631292, 15)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =============================================================================\n# 3. DEEP HOLDOUT SPLIT\n# =============================================================================\nprint(\"\\n--- Creating Deep Holdout Split ---\")\nN_SPLITS = 5\ntscv = TimeSeriesSplit(n_splits=N_SPLITS)\nall_splits = list(tscv.split(X))\n\n# We train on the first N-1 folds and hold out the very last one\ntrain_indices = np.concatenate([all_splits[i][0] for i in range(N_SPLITS - 1)])\nholdout_indices = all_splits[N_SPLITS - 1][1]\n\nX_train, X_holdout = X.iloc[train_indices], X.iloc[holdout_indices]\ny_train, y_holdout = y.iloc[train_indices], y.iloc[holdout_indices]\n\nprint(f\"Training set size: {len(X_train)}\")\nprint(f\"Holdout set size: {len(X_holdout)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T03:56:04.898986Z","iopub.execute_input":"2025-08-15T03:56:04.899264Z","iopub.status.idle":"2025-08-15T03:56:04.988600Z","shell.execute_reply.started":"2025-08-15T03:56:04.899215Z","shell.execute_reply":"2025-08-15T03:56:04.987921Z"}},"outputs":[{"name":"stdout","text":"\n--- Creating Deep Holdout Split ---\nTraining set size: 1052158\nHoldout set size: 105215\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =============================================================================\n# 4. TRAIN THE ENSEMBLE ON FOLDS 0-3\n# =============================================================================\n# --- Scale features ---\nfeature_scaler = MinMaxScaler()\nX_train_scaled = feature_scaler.fit_transform(X_train)\n\n# --- Train LGBM Specialist ---\nprint(\"\\n--- Training Specialist LGBM on Folds 0-3... ---\")\nlgbm_params = {\n    'learning_rate': 0.03, 'num_leaves': 64, 'max_depth': 7,\n    'objective': 'regression_l1', 'n_estimators': 1500, 'random_state': 42,\n    'n_jobs': -1, 'colsample_bytree': 0.7, 'subsample': 0.8\n}\nlgbm_model = lgb.LGBMRegressor(**lgbm_params)\nlgbm_model.fit(X_train_scaled, y_train)\n\n# --- Train LSTM Corrector ---\nprint(\"--- Training Corrector LSTM on in-sample residuals... ---\")\nlgbm_train_preds = lgbm_model.predict(X_train_scaled)\ntrain_residuals = y_train - lgbm_train_preds\n\nresidual_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_residuals_scaled = residual_scaler.fit_transform(train_residuals.values.reshape(-1, 1))\n\ndef create_error_sequences(data, look_back=50):\n    X_seq, y_seq = [], []\n    for i in range(len(data) - look_back):\n        X_seq.append(data[i:(i + look_back), 0])\n        y_seq.append(data[i + look_back, 0])\n    return np.array(X_seq), np.array(y_seq)\n\nLOOK_BACK = 50\nX_residuals, y_residuals = create_error_sequences(train_residuals_scaled, LOOK_BACK)\nX_residuals = np.reshape(X_residuals, (X_residuals.shape[0], X_residuals.shape[1], 1))\n\nlstm_corrector = Sequential([\n    LSTM(units=30, return_sequences=True, input_shape=(LOOK_BACK, 1)), Dropout(0.2),\n    LSTM(units=30), Dropout(0.2), Dense(units=1)\n])\nlstm_corrector.compile(optimizer='adam', loss='mean_squared_error')\nlstm_corrector.fit(X_residuals, y_residuals, epochs=20, batch_size=1024, verbose=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T03:56:04.990121Z","iopub.execute_input":"2025-08-15T03:56:04.990358Z","iopub.status.idle":"2025-08-15T04:03:04.169386Z","shell.execute_reply.started":"2025-08-15T03:56:04.990335Z","shell.execute_reply":"2025-08-15T04:03:04.168741Z"}},"outputs":[{"name":"stdout","text":"\n--- Training Specialist LGBM on Folds 0-3... ---\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3086\n[LightGBM] [Info] Number of data points in the train set: 1052158, number of used features: 13\n[LightGBM] [Info] Start training from score 0.000040\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n--- Training Corrector LSTM on in-sample residuals... ---\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755230282.398813      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755230287.720964     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 14ms/step - loss: 0.0194\nEpoch 2/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0039\nEpoch 3/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0019\nEpoch 4/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 9.6345e-04\nEpoch 5/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 6.0747e-04\nEpoch 6/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 5.3314e-04\nEpoch 7/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 5.0192e-04\nEpoch 8/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 4.7707e-04\nEpoch 9/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 4.6213e-04\nEpoch 10/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 4.3961e-04\nEpoch 11/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 4.2913e-04\nEpoch 12/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 4.2442e-04\nEpoch 13/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 4.0864e-04\nEpoch 14/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 4.0155e-04\nEpoch 15/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 3.9391e-04\nEpoch 16/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 3.9020e-04\nEpoch 17/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 3.7699e-04\nEpoch 18/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 3.8456e-04\nEpoch 19/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 3.7679e-04\nEpoch 20/20\n\u001b[1m1028/1028\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 3.6721e-04\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7df51c61b910>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# =============================================================================\n# 5. DIAGNOSTIC RUN ON A SMALL SAMPLE\n# =============================================================================\nprint(\"\\n--- Running a short diagnostic on the holdout set... ---\")\n\n# --- Predict with LGBM (this is fast) ---\nX_holdout_scaled = feature_scaler.transform(X_holdout)\nholdout_lgbm_preds = lgbm_model.predict(X_holdout_scaled)\n\n# --- Predict corrections with LSTM ---\ninitial_sequence = train_residuals_scaled[-LOOK_BACK:]\ncurrent_sequence = initial_sequence.reshape(1, LOOK_BACK, 1)\nholdout_corrections_scaled = []\n\n# +++ DIAGNOSTIC CHANGE HERE +++\n# We will only run for 500 steps to measure the speed\nnum_diagnostic_steps = 500\nprint(f\"  - Generating {num_diagnostic_steps} rolling predictions from LSTM to diagnose speed...\")\n\nimport time\nstart_time = time.time()\n\nfor i in range(num_diagnostic_steps):\n    pred = lstm_corrector.predict(current_sequence, verbose=0)\n    holdout_corrections_scaled.append(pred[0, 0])\n    current_sequence = np.append(current_sequence[:, 1:, :], pred.reshape(1, 1, 1), axis=1)\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"\\n--- Diagnostic Complete ---\")\nprint(f\"Time to perform {num_diagnostic_steps} predictions: {elapsed_time:.2f} seconds.\")\n\n# --- Extrapolate the full runtime ---\ntime_per_step = elapsed_time / num_diagnostic_steps\nestimated_full_runtime_seconds = time_per_step * len(X_holdout)\nestimated_full_runtime_minutes = estimated_full_runtime_seconds / 60\n\nprint(f\"Average time per prediction: {time_per_step*1000:.2f} milliseconds.\")\nprint(f\"Estimated runtime for the FULL holdout set: {estimated_full_runtime_minutes:.1f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T04:03:04.170058Z","iopub.execute_input":"2025-08-15T04:03:04.170323Z","iopub.status.idle":"2025-08-15T04:03:44.107832Z","shell.execute_reply.started":"2025-08-15T04:03:04.170303Z","shell.execute_reply":"2025-08-15T04:03:44.107120Z"}},"outputs":[{"name":"stdout","text":"\n--- Running a short diagnostic on the holdout set... ---\n  - Generating 500 rolling predictions from LSTM to diagnose speed...\n\n--- Diagnostic Complete ---\nTime to perform 500 predictions: 34.29 seconds.\nAverage time per prediction: 68.58 milliseconds.\nEstimated runtime for the FULL holdout set: 120.3 minutes.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =============================================================================\n# 5. PREDICT & EVALUATE ON HOLDOUT SET (FOLD 4) - CORRECTED\n# =============================================================================\nprint(\"\\n--- Evaluating pipeline on the Deep Holdout Set... ---\")\n\n# --- Predict with LGBM ---\nprint(\"  - Scaling holdout features...\")\nX_holdout_scaled = feature_scaler.transform(X_holdout)\nprint(f\"    Holdout scaled features shape: {X_holdout_scaled.shape}\")\n\nprint(\"  - Predicting with LGBM...\")\nholdout_lgbm_preds = lgbm_model.predict(X_holdout_scaled)\nprint(f\"    LGBM predictions shape: {holdout_lgbm_preds.shape}\")\nprint(f\"    Sample LGBM preds: {holdout_lgbm_preds[:5]}\")\n\n# --- Predict corrections with LSTM ---\nprint(\"  - Preparing initial LSTM sequence from train residuals...\")\ninitial_sequence = train_residuals_scaled[-LOOK_BACK:]\nprint(f\"    Initial sequence shape: {initial_sequence.shape}\")\nprint(f\"    Initial sequence sample: {initial_sequence[:5].flatten()}\")\n\ncurrent_sequence = initial_sequence.reshape(1, LOOK_BACK, 1)\nholdout_corrections_scaled = []\n\nprint(\"  - Generating rolling predictions from LSTM...\")\nfor i in range(len(X_holdout_scaled)):\n    pred = lstm_corrector.predict(current_sequence, verbose=0)\n    holdout_corrections_scaled.append(pred[0, 0])\n\n    if i < 3:  # Print only first few steps\n        print(f\"    Step {i+1} | LSTM pred (scaled): {pred[0,0]:.6f} | Current seq shape: {current_sequence.shape}\")\n    \n    # +++ CORRECTED LINE +++\n    current_sequence = np.append(current_sequence[:, 1:, :], pred.reshape(1, 1, 1), axis=1)\n\nholdout_corrections_scaled = np.array(holdout_corrections_scaled)\nprint(f\"  - LSTM scaled corrections shape: {holdout_corrections_scaled.shape}\")\nprint(f\"    Sample scaled corrections: {holdout_corrections_scaled[:5]}\")\n\n# Inverse transform corrections\nholdout_corrections = residual_scaler.inverse_transform(\n    holdout_corrections_scaled.reshape(-1, 1)\n).flatten()\nprint(f\"  - LSTM corrections (original scale) shape: {holdout_corrections.shape}\")\nprint(f\"    Sample corrections: {holdout_corrections[:5]}\")\n\n# --- Ensemble and Calculate Final Score ---\nprint(\"  - Combining LGBM predictions and LSTM corrections...\")\nholdout_ensemble_preds = holdout_lgbm_preds + holdout_corrections\nprint(f\"    Ensemble predictions shape: {holdout_ensemble_preds.shape}\")\nprint(f\"    Sample ensemble preds: {holdout_ensemble_preds[:5]}\")\n\nholdout_corr, _ = pearsonr(y_holdout, holdout_ensemble_preds)\n\nprint(\"\\n======================================================\")\nprint(f\"ğŸ† Final Pearson Correlation on Deep Holdout Set: {holdout_corr:.5f}\")\nprint(\"======================================================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T04:03:44.108686Z","iopub.execute_input":"2025-08-15T04:03:44.108942Z"}},"outputs":[{"name":"stdout","text":"\n--- Evaluating pipeline on the Deep Holdout Set... ---\n  - Scaling holdout features...\n    Holdout scaled features shape: (105215, 14)\n  - Predicting with LGBM...\n    LGBM predictions shape: (105215,)\n    Sample LGBM preds: [0.00014931 0.00013936 0.00017869 0.0001564  0.00013623]\n  - Preparing initial LSTM sequence from train residuals...\n    Initial sequence shape: (50, 1)\n    Initial sequence sample: [-0.70334119 -0.7078674  -0.70798528 -0.7072703  -0.73312855]\n  - Generating rolling predictions from LSTM...\n    Step 1 | LSTM pred (scaled): -0.716273 | Current seq shape: (1, 50, 1)\n    Step 2 | LSTM pred (scaled): -0.715065 | Current seq shape: (1, 50, 1)\n    Step 3 | LSTM pred (scaled): -0.719794 | Current seq shape: (1, 50, 1)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\n# --- 2. Predictions vs. Actuals Scatter Plot ---\nplt.figure(figsize=(10, 10))\nplt.scatter(y_holdout, holdout_ensemble_preds, alpha=0.1, s=10)\nplt.plot([y_holdout.min(), y_holdout.max()], [y_holdout.min(), y_holdout.max()], 'r--', lw=2, label='Perfect Correlation')\nplt.xlabel('True Values (Implied Volatility)', fontsize=12)\nplt.ylabel('Ensemble Predictions', fontsize=12)\nplt.title(f'Predictions vs. Actuals on Holdout Set\\n(Correlation: {holdout_corr:.4f})', fontsize=16)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / \"predictions_vs_actuals.png\")\nprint(\"âœ… Predictions vs. Actuals plot saved.\")\n\n# --- 3. Residuals Plot (Time Series) ---\nresiduals = y_holdout - holdout_ensemble_preds\nplt.figure(figsize=(15, 6))\nplt.plot(residuals.values, lw=0.5)\nplt.title('Residuals (True - Prediction) on Holdout Set', fontsize=16)\nplt.xlabel('Time Step in Holdout Set', fontsize=12)\nplt.ylabel('Error', fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / \"residuals_plot.png\")\nprint(\"âœ… Residuals plot saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# 6. VISUALIZATIONS\n# =============================================================================\nprint(\"\\n--- Generating Visualizations ---\")\n\n# --- 1. Feature Importance ---\nplt.figure(figsize=(10, 8))\nlgb.plot_importance(lgbm_model, max_num_features=20, height=0.8, ax=plt.gca())\nplt.title('LGBM Model - Top 20 Feature Importances', fontsize=16)\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / \"feature_importance.png\")\nprint(\"âœ… Feature importance plot saved.\")\n\n# --- 2. Predictions vs. Actuals Scatter Plot ---\nplt.figure(figsize=(10, 10))\nplt.scatter(y_holdout, holdout_ensemble_preds, alpha=0.1, s=10)\nplt.plot([y_holdout.min(), y_holdout.max()], [y_holdout.min(), y_holdout.max()], 'r--', lw=2, label='Perfect Correlation')\nplt.xlabel('True Values (Implied Volatility)', fontsize=12)\nplt.ylabel('Ensemble Predictions', fontsize=12)\nplt.title(f'Predictions vs. Actuals on Holdout Set\\n(Correlation: {holdout_corr:.4f})', fontsize=16)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / \"predictions_vs_actuals.png\")\nprint(\"âœ… Predictions vs. Actuals plot saved.\")\n\n# --- 3. Residuals Plot (Time Series) ---\nresiduals = y_holdout - holdout_ensemble_preds\nplt.figure(figsize=(15, 6))\nplt.plot(residuals.values, lw=0.5)\nplt.title('Residuals (True - Prediction) on Holdout Set', fontsize=16)\nplt.xlabel('Time Step in Holdout Set', fontsize=12)\nplt.ylabel('Error', fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / \"residuals_plot.png\")\nprint(\"âœ… Residuals plot saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}